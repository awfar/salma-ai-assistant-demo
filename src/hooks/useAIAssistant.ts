
import { useState, useCallback, useRef } from "react";
import { useToast } from "./use-toast";

interface TextToSpeechCallbacks {
  onStart?: () => void;
  onEnd?: () => void;
}

interface UseAIAssistantReturn {
  askAssistant: (question: string) => Promise<string | null>;
  textToSpeech: (text: string, callbacks?: TextToSpeechCallbacks) => Promise<string | null>;
  isLoading: boolean;
  isAudioLoading: boolean;
  cancelRequest?: () => void;
}

export const useAIAssistant = (): UseAIAssistantReturn => {
  const [isLoading, setIsLoading] = useState(false);
  const [isAudioLoading, setIsAudioLoading] = useState(false);
  const { toast } = useToast();
  const abortControllerRef = useRef<AbortController | null>(null);
  
  // Cancel any in-progress requests
  const cancelRequest = useCallback(() => {
    if (abortControllerRef.current) {
      console.log("ğŸ›‘ Cancelling in-progress AI request");
      abortControllerRef.current.abort();
      abortControllerRef.current = null;
      setIsLoading(false);
    }
  }, []);
  
  // Ask the AI assistant
  const askAssistant = useCallback(async (question: string): Promise<string | null> => {
    // Cancel any existing request
    cancelRequest();
    
    // Create a new abort controller
    abortControllerRef.current = new AbortController();
    const signal = abortControllerRef.current.signal;
    
    try {
      setIsLoading(true);
      
      // Simulate network delay for demo
      await new Promise(resolve => setTimeout(resolve, 500));
      
      // Mock response for demo - in a real app, you would call an API
      const responses = {
        "Ù…Ø§ Ù‡Ùˆ Ù…ÙˆØ¹Ø¯ ØµØ±Ù Ø§Ù„Ù…Ø¹Ø§Ø´ØŸ": "ØªÙØµØ±Ù Ù…Ø¹Ø§Ø´Ø§Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¶Ø§Ù…Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø®Ø§Ù…Ø³ Ù…Ù† ÙƒÙ„ Ø´Ù‡Ø±. ÙˆØ¥Ø°Ø§ ÙˆØ§ÙÙ‚ Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ… Ø¹Ø·Ù„Ø© Ø±Ø³Ù…ÙŠØ©ØŒ ÙŠØªÙ… Ø§Ù„ØµØ±Ù ÙÙŠ ÙŠÙˆÙ… Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ø´ Ù…Ù† Ø£ÙŠ Ù…ÙƒØªØ¨ Ø¨Ø±ÙŠØ¯ Ø£Ùˆ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ù…Ø§ÙƒÙŠÙ†Ø§Øª Ø§Ù„Ø¨Ù†ÙƒÙŠØ© Ø¨Ø§Ù„Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©.",
        "ÙƒÙŠÙ Ø£Ù‚Ø¯Ù… Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ø´ ØªÙƒØ§ÙÙ„ ÙˆÙƒØ±Ø§Ù…Ø©ØŸ": "Ù„Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ø´ ØªÙƒØ§ÙÙ„ ÙˆÙƒØ±Ø§Ù…Ø©ØŒ ÙŠØ¬Ø¨ Ø§ØªØ¨Ø§Ø¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:\n1. Ø§Ù„ØªÙˆØ¬Ù‡ Ù„Ø£Ù‚Ø±Ø¨ ÙˆØ­Ø¯Ø© Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ© ØªØ§Ø¨Ø¹Ø© Ù„ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¶Ø§Ù…Ù†\n2. Ù…Ù„Ø¡ Ø§Ø³ØªÙ…Ø§Ø±Ø© Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…\n3. ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: ØµÙˆØ±Ø© Ø§Ù„Ø¨Ø·Ø§Ù‚Ø©ØŒ ÙˆØ´Ù‡Ø§Ø¯Ø© Ù…ÙŠÙ„Ø§Ø¯ Ø§Ù„Ø£Ø·ÙØ§Ù„ Ø¥Ù† ÙˆØ¬Ø¯ÙˆØ§ØŒ ÙˆØ¥Ø«Ø¨Ø§Øª Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©\n4. Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆØ¥Ø¹Ù„Ø§Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬",
        "Ù…Ø§ Ù‡ÙŠ Ø´Ø±ÙˆØ· Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ø´ ØªÙƒØ§ÙÙ„ØŸ": "Ø´Ø±ÙˆØ· Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ø´ ØªÙƒØ§ÙÙ„ ØªØ´Ù…Ù„:\n1. Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø£Ø³Ø±Ø© ÙÙ‚ÙŠØ±Ø© Ø­Ø³Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª Ù‚ÙŠØ§Ø³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø¹ÙŠØ´Ø©\n2. Ø£Ù† ÙŠÙƒÙˆÙ† Ù„Ø¯Ù‰ Ø§Ù„Ø£Ø³Ø±Ø© Ø£Ø·ÙØ§Ù„ ÙÙŠ Ø³Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ… (0-18 Ø³Ù†Ø©)\n3. Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙˆØ§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„ØµØ­ÙŠØ© Ù„Ù„Ø£Ø·ÙØ§Ù„\n4. Ø£Ù„Ø§ ÙŠÙƒÙˆÙ† Ø£ÙŠ Ù…Ù† Ø§Ù„Ø²ÙˆØ¬ÙŠÙ† Ù…Ø³ØªÙÙŠØ¯Ø§Ù‹ Ù…Ù† Ø£ÙŠ Ù…Ø¹Ø§Ø´ Ø¢Ø®Ø±\n5. ØªØ¹Ø·Ù‰ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø£Ø³Ø± Ø§Ù„ØªÙŠ ØªØ¹ÙˆÙ„Ù‡Ø§ Ù†Ø³Ø§Ø¡ ÙˆØ§Ù„Ø£Ø³Ø± Ø§Ù„ØªÙŠ Ù„Ø¯ÙŠÙ‡Ø§ Ø£Ø·ÙØ§Ù„ Ø°ÙˆÙŠ Ø¥Ø¹Ø§Ù‚Ø©",
      };
      
      // Default response for unrecognized questions
      let response = "Ø£Ø¹ØªØ°Ø±ØŒ Ù„ÙŠØ³ Ù„Ø¯ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¶Ø§Ù…Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù‚Ù… 16439 Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©.";
      
      // Check if we have a specific response for this question
      Object.entries(responses).forEach(([key, value]) => {
        if (question.includes(key) || key.includes(question)) {
          response = value;
        }
      });
      
      // If request was cancelled, return null
      if (signal.aborted) {
        console.log("ğŸ›‘ Request was cancelled");
        return null;
      }
      
      return response;
    } catch (error) {
      console.error("Error asking assistant:", error);
      
      if (error instanceof Error && error.name === "AbortError") {
        console.log("ğŸ›‘ Request was aborted");
        return null;
      }
      
      toast({
        title: "Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„",
        description: "Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
        variant: "destructive",
      });
      
      return null;
    } finally {
      setIsLoading(false);
    }
  }, [toast, cancelRequest]);
  
  // Convert text to speech
  const textToSpeech = useCallback(async (text: string, callbacks?: TextToSpeechCallbacks): Promise<string | null> => {
    try {
      setIsAudioLoading(true);
      callbacks?.onStart && callbacks.onStart();
      
      // Simulate TTS service delay
      await new Promise(resolve => setTimeout(resolve, 800));
      
      // In a real app, you would call a TTS service like ElevenLabs or Google TTS
      // For the demo, we'll use the browser's built-in speech synthesis API
      const url = `data:audio/mp3;base64,SUQzBAAAAAACH1RJVDIAAAAeAAAAVGV4dCB0byBTcGVlY2ggQXVkaW8gU2FtcGxlVFhYWAAAAB9BcnRpZmljaWFsIGZpbGUgZm9yIGRlbW8gcHVycG9zZXNUQUxCAAAAFlRleHQgdG8gU3BlZWNoIFNhbXBsZXNHRU9CAAAABFVTQRBUT0ZMAAAAFVN5bnRoZXNpemVkIHNwZWVjaA==`;
      
      return url;
    } catch (error) {
      console.error("Error in text to speech:", error);
      toast({
        title: "Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª",
        description: "Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø¯ Ø¥Ù„Ù‰ ØµÙˆØª. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
        variant: "destructive",
      });
      return null;
    } finally {
      setIsAudioLoading(false);
      // We'll let the callback handle the onEnd event
    }
  }, [toast]);
  
  return { askAssistant, textToSpeech, isLoading, isAudioLoading, cancelRequest };
};
