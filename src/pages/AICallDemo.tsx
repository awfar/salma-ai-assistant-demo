
import React, { useEffect } from "react";
import { useNavigate } from "react-router-dom";
import { Settings } from "lucide-react";
import { Button } from "@/components/ui/button";
import { useToast } from "@/hooks/use-toast";
import CallHeader from "@/components/CallHeader";
import CallStartScreen from "@/components/CallStartScreen";
import ActiveCallScreen from "@/components/ActiveCallScreen";
import CallFooter from "@/components/CallFooter";

const AICallDemo = () => {
  const navigate = useNavigate();
  const [callActive, setCallActive] = React.useState(false);
  const [callStartTime, setCallStartTime] = React.useState<Date>(new Date());
  const [micPermissionGranted, setMicPermissionGranted] = React.useState(false);
  const { toast } = useToast();
  
  // Check for microphone permission on load - with improved initialization
  useEffect(() => {
    const checkMicPermission = async () => {
      try {
        console.log("ğŸ¤ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†...");
        // Use a more direct approach to check microphone permissions
        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          } 
        });
        
        console.log("âœ… ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ø¨Ù†Ø¬Ø§Ø­");
        setMicPermissionGranted(true);
        
        // Test that we're actually getting audio
        const audioContext = new AudioContext();
        const source = audioContext.createMediaStreamSource(stream);
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
        source.connect(analyser);
        
        // Test for audio level
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        analyser.getByteFrequencyData(dataArray);
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
          sum += dataArray[i];
        }
        
        const average = sum / dataArray.length;
        console.log("ğŸ¤ Ù…Ø³ØªÙˆÙ‰ ØµÙˆØª Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ø§Ù„Ø£ÙˆÙ„ÙŠ:", average);
        
        // Clean up the test stream
        source.disconnect();
        stream.getTracks().forEach(track => track.stop());
        audioContext.close();
      } catch (err) {
        console.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„:", err);
        toast({
          title: "ØªØ­Ø°ÙŠØ±",
          description: "Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø©.",
          variant: "destructive",
          duration: 5000,
        });
      }
    };
    
    checkMicPermission();
  }, [toast]);
  
  // Handle starting call - with more aggressive mic handling
  const handleStartCallClick = async () => {
    try {
      console.log("ğŸ¤ Ø·Ù„Ø¨ Ø¥Ø°Ù† Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙˆØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©...");
      
      // Create audio context and attempt to warm it up
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      
      // Request microphone permission with optimized settings
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 48000,
        } 
      });
      
      // Create an analyzer and check audio levels
      const analyser = audioContext.createAnalyser();
      const source = audioContext.createMediaStreamSource(stream);
      analyser.fftSize = 1024; // Higher resolution
      source.connect(analyser);
      
      // Check audio input is working with better sensitivity
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      
      // Sample audio multiple times to ensure we're getting data
      let hasAudio = false;
      for (let attempt = 0; attempt < 5; attempt++) {
        analyser.getByteFrequencyData(dataArray);
        
        // Calculate average level
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
          sum += dataArray[i];
        }
        
        const average = sum / dataArray.length;
        console.log(`ğŸ¤ Test microphone level (attempt ${attempt+1}):`, average);
        
        if (average > 0) {
          hasAudio = true;
          break;
        }
        
        // Wait a bit before next sample
        await new Promise(resolve => setTimeout(resolve, 100));
      }
      
      // Clean up test resources
      source.disconnect();
      stream.getTracks().forEach(track => track.stop());
      audioContext.close();
      
      // Start call
      setMicPermissionGranted(true);
      setCallActive(true);
      setCallStartTime(new Date());
      
      toast({
        title: "Ø¨Ø¯Ø¡ Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©",
        description: "ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø³Ù„Ù…Ù‰",
        duration: 2000,
      });
      
    } catch (err) {
      console.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†:", err);
      toast({
        title: "Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†",
        description: "ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ",
        variant: "destructive",
        duration: 5000,
      });
    }
  };

  // Handle ending call
  const handleEndCallClick = () => {
    setCallActive(false);
    
    toast({
      title: "ØªÙ… Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©",
      description: "Ø´ÙƒØ±Ø§Ù‹ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ Ø®Ø¯Ù…Ø© Ø³Ù„Ù…Ù‰ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ",
      duration: 3000,
    });
  };

  // Handle settings click
  const handleSettingsClick = () => {
    navigate('/ai-settings');
  };

  return (
    <div className="flex flex-col min-h-screen bg-ministry-light">
      {/* Header with ministry logo */}
      <CallHeader onSettingsClick={handleSettingsClick} />

      {/* Main content */}
      <div className="flex-1 flex flex-col md:flex-row relative w-full max-w-7xl mx-auto">
        <div className="flex-1 flex flex-col items-center justify-center md:p-6">
          {!callActive ? (
            <CallStartScreen onStartCall={handleStartCallClick} />
          ) : (
            <ActiveCallScreen 
              callStartTime={callStartTime}
              onEndCall={handleEndCallClick}
            />
          )}
        </div>
      </div>

      {/* Footer */}
      <CallFooter />
    </div>
  );
};

export default AICallDemo;
