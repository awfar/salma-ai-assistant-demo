
import React, { useState, useEffect, useRef, useCallback } from "react";
import { PhoneOff, Volume2, Volume } from "lucide-react";
import CallTimer from "@/components/CallTimer";
import AudioPlayer from "@/components/AudioPlayer";
import AvatarAnimation from "@/components/AvatarAnimation";
import CallButton from "@/components/CallButton";
import ChatBubble from "@/components/ChatBubble";
import TranscriptBar from "@/components/TranscriptBar";
import SuggestedQuestions from "@/components/SuggestedQuestions";
import { useAIAssistant } from "@/hooks/useAIAssistant";
import { useToast } from "@/hooks/use-toast";
import { useCallMessages } from "@/hooks/useCallMessages";
import { speechTranscriptionService } from "@/services/speechTranscriptionService";
import PushToTalkButton from "@/components/PushToTalkButton";
import { createVoiceRecorder, VoiceRecorderInterface } from "@/utils/voiceRecorder";

interface ActiveCallScreenProps {
  callStartTime: Date;
  onEndCall: () => void;
}

const ActiveCallScreen: React.FC<ActiveCallScreenProps> = ({ 
  callStartTime, 
  onEndCall 
}) => {
  const [isSpeaking, setIsSpeaking] = useState<boolean>(false);
  const [isRecording, setIsRecording] = useState<boolean>(false);
  const [currentTranscript, setCurrentTranscript] = useState<string>("");
  const [isSpeakerOn, setIsSpeakerOn] = useState<boolean>(true);
  const [audioMuted, setAudioMuted] = useState<boolean>(false);
  const [audioLevel, setAudioLevel] = useState<number>(0);
  const [showErrorMessage, setShowErrorMessage] = useState<boolean>(false);
  const [errorMessage, setErrorMessage] = useState<string>("");
  const [useStreamingAudio, setUseStreamingAudio] = useState<boolean>(true);
  const { toast } = useToast();
  
  // References
  const chatContainerRef = useRef<HTMLDivElement>(null);
  const audioControllerRef = useRef<{ 
    pause: () => void;
    play: () => Promise<void>;
    isPlaying: boolean; 
  } | null>(null);
  const audioStreamRef = useRef<MediaSource | AudioBufferSourceNode | null>(null);
  const firstMessagePlayed = useRef<boolean>(false);
  const welcomeAttempted = useRef<boolean>(false);
  const recorderRef = useRef<VoiceRecorderInterface | null>(null);
  const processingUserInputRef = useRef<boolean>(false);
  const noSpeechTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const errorTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  
  // The AI assistant hook
  const { 
    askAssistant, 
    textToSpeech,
    streamToSpeech,
    cancelRequest,
    isLoading: isAIThinking,
    isAudioLoading 
  } = useAIAssistant();
  
  // Call messages state management
  const { messages, addMessage } = useCallMessages();
  
  // The suggested questions
  const [suggestedQuestions] = useState<string[]>([
    "Ø¥ÙŠÙ‡ Ù‡Ùˆ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªÙƒØ§ÙÙ„ ÙˆÙƒØ±Ø§Ù…Ø©ØŸ",
    "Ø¥Ø²Ø§ÙŠ Ø£Ù‚Ø¯Ù… ÙÙŠ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ØŸ",
    "Ø¥ÙŠÙ‡ Ù‡ÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŸ",
    "Ø£Ù†Ø§ Ø£Ø±Ù…Ù„Ø©.. Ù‡Ù„ Ù…Ù…ÙƒÙ† Ø£Ø³ØªÙÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ØŸ",
    "Ù…ÙˆØ§Ø¹ÙŠØ¯ ØµØ±Ù Ø§Ù„Ø¯Ø¹Ù… Ø¥Ù…ØªÙ‰ØŸ",
    "Ø¹Ù†Ø¯ÙŠ Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙƒØ±Ø§Ù…Ø©",
  ]);

  // Stop current audio playback and reset speech state
  const stopCurrentAudio = useCallback(() => {
    if (audioControllerRef.current && audioControllerRef.current.isPlaying) {
      console.log("ğŸ›‘ Ø¥ÙŠÙ‚Ø§Ù ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø§Ù„Ø­Ø§Ù„ÙŠ");
      audioControllerRef.current.pause();
      setIsSpeaking(false);
    }
  }, []);

  // Show error message with auto-dismiss
  const showError = useCallback((message: string, duration: number = 5000) => {
    setErrorMessage(message);
    setShowErrorMessage(true);
    
    // Clear any existing timeout
    if (errorTimeoutRef.current) {
      clearTimeout(errorTimeoutRef.current);
    }
    
    // Auto-dismiss after duration
    errorTimeoutRef.current = setTimeout(() => {
      setShowErrorMessage(false);
      setErrorMessage("");
      errorTimeoutRef.current = null;
    }, duration);
  }, []);

  // Initialize the voice recorder
  useEffect(() => {
    console.log("ğŸ¤ ØªÙ‡ÙŠØ¦Ø© Ù…Ø³Ø¬Ù„ Ø§Ù„ØµÙˆØª...");
    recorderRef.current = createVoiceRecorder({
      onAudioLevel: (level) => {
        setAudioLevel(level);
      }
    });

    return () => {
      // Clean up
      if (recorderRef.current && recorderRef.current.isRecording()) {
        recorderRef.current.cancelRecording();
      }
      
      // Clear error timeout
      if (errorTimeoutRef.current) {
        clearTimeout(errorTimeoutRef.current);
        errorTimeoutRef.current = null;
      }
    };
  }, []);

  // Process user input from voice recording or text
  const processUserInput = async (text: string) => {
    if (!text.trim() || processingUserInputRef.current) return;
    
    try {
      console.log("ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:", text);
      processingUserInputRef.current = true;
      
      // Stop any current audio
      stopCurrentAudio();
      
      // Cancel any pending requests
      cancelRequest?.();
      
      // Hide any error messages
      setShowErrorMessage(false);
      
      // Show current transcript
      setCurrentTranscript(text.trim());
      
      // Add user message
      console.log("ğŸ‘¤ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:", text.trim());
      addMessage(text.trim(), "user");
      
      // Get response from AI assistant
      console.log("ğŸ¤– Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ...");
      const aiResponse = await askAssistant(text.trim());
      
      if (aiResponse) {
        console.log("ğŸ¤– ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ:", aiResponse);
        
        // Add assistant response
        addMessage(aiResponse, "assistant");
        
        // Convert text to speech
        if (!audioMuted && isSpeakerOn) {
          console.log("ğŸ”Š ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…...");
          
          if (useStreamingAudio) {
            // Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¯ÙÙ‚ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            console.log("ğŸ”Š Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© ØªØ¯ÙÙ‚ Ø§Ù„ØµÙˆØª ElevenLabs...");
            setIsSpeaking(true);
            
            try {
              await streamToSpeech(aiResponse, {
                onStart: () => {
                  console.log("ğŸ”Š Ø¨Ø¯Ø¡ ØªØ¯ÙÙ‚ Ø§Ù„ØµÙˆØª");
                  setIsSpeaking(true);
                },
                onStreamStart: (source) => {
                  console.log("ğŸ”Š ØªÙ… Ø¨Ø¯Ø¡ Ù…ØµØ¯Ø± Ø§Ù„ØµÙˆØª");
                  audioStreamRef.current = source;
                },
                onChunk: (chunk) => {
                  // ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªØªØ¨Ø¹ ØªÙ‚Ø¯Ù… Ø§Ù„Ø¯ÙÙ‚ Ù‡Ù†Ø§ Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Ù†Ø§ Ù„Ø°Ù„Ùƒ
                },
                onEnd: () => {
                  console.log("ğŸ”Š Ø§Ù†ØªÙ‡Ù‰ ØªØ¯ÙÙ‚ Ø§Ù„ØµÙˆØª");
                  audioStreamRef.current = null;
                  setIsSpeaking(false);
                  handleAudioEnded();
                }
              });
            } catch (e) {
              console.error("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ¯ÙÙ‚ Ø§Ù„ØµÙˆØª:", e);
              setIsSpeaking(false);
              handleAudioEnded();
              
              // Try falling back to regular audio if streaming fails
              console.log("ğŸ”„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© Ø¨Ø¹Ø¯ ÙØ´Ù„ Ø§Ù„Ø¯ÙÙ‚...");
              
              const audioUrl = await textToSpeech(aiResponse, {
                onStart: () => {
                  console.log("ğŸ”Š Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª (Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©)");
                  setIsSpeaking(true);
                },
                onEnd: () => {
                  console.log("ğŸ”Š Ø§Ù†ØªÙ‡Ù‰ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª (Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©)");
                  setIsSpeaking(false);
                  handleAudioEnded();
                }
              });
              
              if (audioUrl) {
                console.log("ğŸ”Š ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØª:", audioUrl.substring(0, 50) + "...");
                setAudioSource(audioUrl);
              } else {
                console.error("âŒ ÙØ´Ù„ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØª");
                handleAudioEnded(); 
              }
            }
            
          } else {
            // Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù ØµÙˆØªÙŠ ÙƒØ§Ù…Ù„
            const audioUrl = await textToSpeech(aiResponse, {
              onStart: () => {
                console.log("ğŸ”Š Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª");
                setIsSpeaking(true);
              },
              onEnd: () => {
                console.log("ğŸ”Š Ø§Ù†ØªÙ‡Ù‰ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª");
                setIsSpeaking(false);
                handleAudioEnded();
              }
            });
            
            if (audioUrl) {
              console.log("ğŸ”Š ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØª:", audioUrl.substring(0, 50) + "...");
              setAudioSource(audioUrl);
            } else {
              console.error("âŒ ÙØ´Ù„ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØª");
              handleAudioEnded(); 
            }
          }
        } else {
          // If sound is disabled, skip audio phase
          console.log("ğŸ”‡ ØªØ®Ø·ÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª (Ù…ÙƒØªÙˆÙ… Ø£Ùˆ ØºÙŠØ± Ù†Ø´Ø·)");
          handleAudioEnded();
        }
      } else {
        console.error("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ");
        toast({
          title: "Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯",
          description: "Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
          variant: "destructive",
        });
        handleAudioEnded();
      }
    } catch (error) {
      console.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª:", error);
      toast({
        title: "Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª",
        description: error instanceof Error ? error.message : "Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹",
        variant: "destructive",
      });
      handleAudioEnded();
    } finally {
      processingUserInputRef.current = false;
    }
  };
  
  // Handle start of recording
  const handleStartRecording = useCallback(async () => {
    try {
      // If we're already processing or speaking, stop first
      if (processingUserInputRef.current || isSpeaking) {
        console.log("ğŸ›‘ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¬Ø§Ø±ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯");
        stopCurrentAudio();
        cancelRequest?.();
        // Give a moment for cleanup before starting
        await new Promise(resolve => setTimeout(resolve, 100));
      }
      
      // Hide any error messages
      setShowErrorMessage(false);
      
      // Start recording
      if (!recorderRef.current) {
        recorderRef.current = createVoiceRecorder({
          onAudioLevel: (level) => {
            setAudioLevel(level);
          }
        });
      }
      
      console.log("ğŸ¤ Ø¨Ø¯Ø¡ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª...");
      await recorderRef.current.startRecording();
      setIsRecording(true);
      setCurrentTranscript("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹...");
      
      // Set a timeout to check if no speech is detected
      noSpeechTimeoutRef.current = setTimeout(() => {
        if (audioLevel < 0.1 && isRecording) {
          console.log("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙƒÙ„Ø§Ù… Ø®Ù„Ø§Ù„ ÙØªØ±Ø© Ø§Ù„Ù…Ù‡Ù„Ø©");
          handleStopRecording(true);
        }
      }, 6500); // Check just before 7 second max recording time

    } catch (err) {
      console.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„:", err);
      setIsRecording(false);
      showError("Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„.", 3000);
    }
  }, [stopCurrentAudio, audioLevel, cancelRequest, showError, isRecording, isSpeaking]);
  
  // Handle end of recording
  const handleStopRecording = useCallback(async (noSpeechDetected = false) => {
    try {
      if (!recorderRef.current || !recorderRef.current.isRecording()) {
        setIsRecording(false);
        return;
      }
      
      // Clear no speech timeout
      if (noSpeechTimeoutRef.current) {
        clearTimeout(noSpeechTimeoutRef.current);
        noSpeechTimeoutRef.current = null;
      }
      
      setIsRecording(false);
      setCurrentTranscript("Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª...");
      
      // Stop the recording and get the audio blob
      console.log("ğŸ¤ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ...");
      const audioBlob = await recorderRef.current.stopRecording();
      
      // If we detected no speech, show a message
      if (noSpeechDetected || audioBlob.size < 1000) {
        console.warn("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙƒÙ„Ø§Ù… Ø£Ùˆ Ø§Ù„ØµÙˆØª ØµØºÙŠØ± Ø¬Ø¯Ù‹Ø§:", audioBlob.size);
        showError("Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªÙ‚Ø§Ø· ØµÙˆØªØŒ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰", 2000);
        return;
      }
      
      console.log(`ğŸ¤ ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ù…Ù„Ù ØµÙˆØªÙŠ: ${audioBlob.size} Ø¨Ø§ÙŠØªØŒ Ù†ÙˆØ¹: ${audioBlob.type}`);
      
      // Transcribe the audio
      try {
        console.log("ğŸ”„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ...");
        const text = await speechTranscriptionService.transcribeAudio(audioBlob);
        
        if (!text) {
          console.error("âŒ Ù„Ù… ÙŠØªÙ… Ø¥Ø±Ø¬Ø§Ø¹ Ø£ÙŠ Ù†Øµ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­ÙˆÙŠÙ„");
          throw new Error("ÙØ´Ù„ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ");
        }
        
        console.log("âœ… Ù†Ø¬Ø­Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­ÙˆÙŠÙ„:", text);
        
        // Process the transcribed text
        await processUserInput(text);
      } catch (transcriptionError) {
        console.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„:", transcriptionError);
        showError(
          transcriptionError instanceof Error 
            ? transcriptionError.message 
            : "Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.", 
          3000
        );
      }
      
    } catch (err) {
      console.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„:", err);
      setIsRecording(false);
      setCurrentTranscript("");
      
      showError("Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØªØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.", 3000);
    }
  }, [processUserInput, showError]);

  // Handle suggested question selection
  const handleQuestionSelect = (question: string) => {
    console.log("ğŸ–±ï¸ ØªÙ… Ø§Ù„Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø³Ø±ÙŠØ¹:", question);
    if (isSpeaking) {
      console.log("ğŸ›‘ Ø¥ÙŠÙ‚Ø§Ù ÙƒÙ„Ø§Ù… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„");
      stopCurrentAudio();
    }
    
    if (isRecording) {
      console.log("ğŸ›‘ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„");
      recorderRef.current?.cancelRecording();
      setIsRecording(false);
    }
    
    // Ø¥Ø¶Ø§ÙØ© ØªØ£Ø®ÙŠØ± Ù‚ØµÙŠØ± Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¹Ø§Ø¯Ø© Ø¶Ø¨Ø· ÙƒÙ„ Ø´ÙŠØ¡
    setTimeout(() => {
      processUserInput(question);
    }, 200);
  };

  // Audio source state
  const [audioSource, setAudioSource] = useState<string | undefined>();

  // When audio ends
  const handleAudioEnded = useCallback(() => {
    setIsSpeaking(false);
    setCurrentTranscript(""); // Clear transcript bar text
    audioStreamRef.current = null;
  }, []);

  // Handle speaker button click - controls audio output
  const handleSpeakerClick = () => {
    const newSpeakerState = !isSpeakerOn;
    setIsSpeakerOn(newSpeakerState);
    setAudioMuted(!newSpeakerState);
    
    toast({
      title: newSpeakerState ? "ØªÙ… ØªØ´ØºÙŠÙ„ Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª" : "ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª",
      duration: 2000,
    });
    
    // Stop current audio if turning speaker off
    if (isSpeakerOn && isSpeaking) {
      stopCurrentAudio();
      handleAudioEnded();
    }
  };

  // Setup audio controller reference
  const setupAudioController = useCallback((controller: { 
    pause: () => void;
    play: () => Promise<void>;
    isPlaying: boolean; 
  } | null) => {
    audioControllerRef.current = controller;
  }, []);

  // Scroll to bottom when messages change
  useEffect(() => {
    if (chatContainerRef.current) {
      chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight;
    }
  }, [messages]);
  
  // Clean up on unmount
  useEffect(() => {
    return () => {
      if (recorderRef.current && recorderRef.current.isRecording()) {
        recorderRef.current.cancelRecording();
      }
      
      stopCurrentAudio();
      cancelRequest?.();
      
      if (noSpeechTimeoutRef.current) {
        clearTimeout(noSpeechTimeoutRef.current);
      }
      
      if (errorTimeoutRef.current) {
        clearTimeout(errorTimeoutRef.current);
      }
    };
  }, [stopCurrentAudio, cancelRequest]);

  // Play welcome message on first render
  useEffect(() => {
    // Don't try to play welcome if we've already done it or attempted it
    if (firstMessagePlayed.current || welcomeAttempted.current) {
      return;
    }

    welcomeAttempted.current = true;
    
    const playWelcomeMessage = async () => {
      try {
        const welcomeMessage = "Ø£Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ÙÙŠ ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¶Ø§Ù…Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŒ Ù…Ø¹Ø§Ùƒ Ø³Ù„Ù…Ù‰ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„Ø°ÙƒÙŠØ© Ø£Ù†Ø§ Ù‡Ù†Ø§ Ø¹Ø´Ø§Ù† Ø§Ø¬Ø§ÙˆØ¨Ùƒ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª. Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙˆØ§Ø³ØªÙ…Ø± Ø¨Ø§Ù„Ø¶ØºØ· Ø¹Ù„ÙŠÙ‡ ÙˆØ§Ù†Øª Ø¨ØªØªÙƒÙ„Ù….";
        console.log("ğŸ¤– ØªØ´ØºÙŠÙ„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨:", welcomeMessage);
        
        // Add message to chat history
        addMessage(welcomeMessage, "assistant");
        
        // Set speaking state to show animation first
        setIsSpeaking(true);
        
        // Only attempt text to speech if speaker is on
        if (isSpeakerOn) {
          console.log("ğŸ”Š ØªØ­ÙˆÙŠÙ„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨ Ø¥Ù„Ù‰ ØµÙˆØª...");
          
          if (useStreamingAudio) {
            // Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¯ÙÙ‚ Ù„Ù„ØªØ±Ø­ÙŠØ¨
            try {
              await streamToSpeech(welcomeMessage, {
                onStart: () => {
                  console.log("ğŸ”Š Ø¨Ø¯Ø¡ ØªØ¯ÙÙ‚ ØµÙˆØª Ø§Ù„ØªØ±Ø­ÙŠØ¨");
                  setIsSpeaking(true);
                },
                onStreamStart: (source) => {
                  audioStreamRef.current = source;
                },
                onChunk: (chunk) => {
                  // ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªØªØ¨Ø¹ ØªÙ‚Ø¯Ù… Ø§Ù„Ø¯ÙÙ‚ Ù‡Ù†Ø§ Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Ù†Ø§ Ù„Ø°Ù„Ùƒ
                },
                onEnd: () => {
                  console.log("ğŸ”Š Ø§Ù†ØªÙ‡Ù‰ ØªØ¯ÙÙ‚ ØµÙˆØª Ø§Ù„ØªØ±Ø­ÙŠØ¨");
                  audioStreamRef.current = null;
                  setIsSpeaking(false);
                  handleAudioEnded();
                  firstMessagePlayed.current = true;
                }
              });
            } catch (e) {
              console.error("âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯ÙÙ‚ ØµÙˆØª Ø§Ù„ØªØ±Ø­ÙŠØ¨:", e);
              
              // Ø§Ù„Ø±Ø¬ÙˆØ¹ Ù„Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„
              const audioUrl = await textToSpeech(welcomeMessage, {
                onStart: () => {
                  console.log("ğŸ”Š Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨ (Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©)");
                  setIsSpeaking(true);
                },
                onEnd: () => {
                  console.log("ğŸ”Š Ø§Ù†ØªÙ‡Øª Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨");
                  setIsSpeaking(false);
                  handleAudioEnded();
                  firstMessagePlayed.current = true;
                }
              });
              
              if (audioUrl) {
                console.log("ğŸ”Š ØªØ¹ÙŠÙŠÙ† Ù…ØµØ¯Ø± ØµÙˆØª Ø§Ù„ØªØ±Ø­ÙŠØ¨");
                setAudioSource(audioUrl);
              } else {
                setIsSpeaking(false);
                handleAudioEnded();
                firstMessagePlayed.current = true;
              }
            }
          } else {
            // Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù ØµÙˆØªÙŠ ÙƒØ§Ù…Ù„
            const audioUrl = await textToSpeech(welcomeMessage, {
              onStart: () => {
                console.log("ğŸ”Š Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨");
                setIsSpeaking(true);
              },
              onEnd: () => {
                console.log("ğŸ”Š Ø§Ù†ØªÙ‡Øª Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨");
                setIsSpeaking(false);
                handleAudioEnded();
                firstMessagePlayed.current = true; // Mark as played after completion
              }
            });
            
            if (audioUrl) {
              console.log("ğŸ”Š ØªØ¹ÙŠÙŠÙ† Ù…ØµØ¯Ø± ØµÙˆØª Ø§Ù„ØªØ±Ø­ÙŠØ¨");
              setAudioSource(audioUrl);
            } else {
              console.error("âŒ ÙØ´Ù„ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· ØµÙˆØª Ø§Ù„ØªØ±Ø­ÙŠØ¨");
              setIsSpeaking(false);
              handleAudioEnded();
              // Still mark as played to avoid retries
              firstMessagePlayed.current = true;
            }
          }
        } else {
          // Just mark as played if speaker is off
          firstMessagePlayed.current = true;
          setIsSpeaking(false);
          handleAudioEnded();
        }
      } catch (error) {
        console.error("âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨:", error);
        setIsSpeaking(false);
        handleAudioEnded();
        // Set as played to prevent retries
        firstMessagePlayed.current = true;
      }
    };
    
    // Play welcome message after a short delay to ensure components are mounted
    const welcomeTimer = setTimeout(playWelcomeMessage, 800);
    
    return () => clearTimeout(welcomeTimer);
  }, [textToSpeech, streamToSpeech, addMessage, isSpeakerOn, handleAudioEnded, useStreamingAudio]);

  return (
    <div className="relative w-full max-w-md mx-auto aspect-[9/16] md:aspect-auto md:h-[80vh] overflow-hidden rounded-2xl md:rounded-3xl bg-black shadow-xl border-8 border-gray-800 flex flex-col">
      {/* Call background */}
      <div className="absolute inset-0 bg-gradient-to-b from-ministry-dark to-black/90"></div>
      
      {/* Status bar at top */}
      <div className="absolute top-0 left-0 right-0 h-12 bg-black/30 backdrop-blur-lg flex items-center justify-center z-10">
        <CallTimer isActive={true} startTime={callStartTime} className="text-white font-bold" />
      </div>
      
      {/* Chat container - hidden in call UI, used for internal tracking */}
      <div 
        ref={chatContainerRef}
        className="opacity-0 absolute inset-0 overflow-y-auto"
      >
        {messages.map((message) => (
          <ChatBubble
            key={message.id}
            message={message.text}
            sender={message.sender}
          />
        ))}
      </div>
      
      {/* Error message display */}
      {showErrorMessage && (
        <div className="absolute top-16 left-4 right-4 z-30">
          <div className="bg-red-500 text-white px-4 py-3 rounded-lg shadow-lg">
            <h3 className="font-bold">Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª</h3>
            <p>{errorMessage || "Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."}</p>
          </div>
        </div>
      )}
      
      {/* Animated avatar - center of screen */}
      <div className="absolute inset-0 flex items-center justify-center pointer-events-none z-10">
        <AvatarAnimation 
          isActive={isSpeaking} 
          isListening={isRecording}
          audioLevel={audioLevel}
        />
      </div>
      
      {/* Current transcript bar - under chin */}
      <div className="absolute bottom-32 left-0 right-0 z-20 px-4">
        <TranscriptBar 
          text={currentTranscript} 
          isActive={Boolean(currentTranscript)} 
          autoHide={false}
        />
      </div>
      
      {/* Suggested questions bar */}
      <div className="absolute bottom-24 left-0 right-0 z-20 px-2">
        <SuggestedQuestions 
          questions={suggestedQuestions} 
          onQuestionSelect={handleQuestionSelect} 
        />
      </div>
      
      {/* Status indicators */}
      {isAIThinking && (
        <div className="absolute bottom-40 left-0 right-0 z-10 flex justify-center">
          <div className="bg-ministry-dark/50 backdrop-blur-sm px-4 py-2 rounded-full flex items-center gap-2">
            <div className="text-white text-xs">Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙÙƒÙŠØ±</div>
            <div className="flex space-x-1 rtl:space-x-reverse">
              {Array.from({ length: 3 }).map((_, i) => (
                <div 
                  key={i} 
                  className="w-1.5 h-1.5 bg-gray-400 rounded-full animate-pulse"
                  style={{ animationDelay: `${i * 200}ms` }}
                ></div>
              ))}
            </div>
          </div>
        </div>
      )}
      
      {/* Audio processing icon */}
      {isAudioLoading && (
        <div className="absolute top-16 left-4 flex items-center gap-2 animate-pulse">
          <div className="flex space-x-1 rtl:space-x-reverse">
            <div className="w-2 h-2 bg-blue-400 rounded-full animate-pulse"></div>
            <div className="w-2 h-2 bg-blue-400 rounded-full animate-pulse" style={{ animationDelay: '200ms' }}></div>
            <div className="w-2 h-2 bg-blue-400 rounded-full animate-pulse" style={{ animationDelay: '400ms' }}></div>
          </div>
          <span className="text-xs text-white bg-blue-400/80 px-2 py-0.5 rounded-full">Ø¬Ø§Ø±ÙŠ ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙˆØª</span>
        </div>
      )}
      
      {/* Push to Talk and Call control buttons */}
      <div className="absolute bottom-4 left-0 right-0 z-30">
        <div className="flex items-center justify-center space-x-5 rtl:space-x-reverse">
          {/* Push to talk button (bigger, in center) */}
          <div className="relative flex-1 flex justify-center">
            <PushToTalkButton 
              onStartRecording={handleStartRecording} 
              onStopRecording={() => handleStopRecording(false)}
              isRecording={isRecording}
              audioLevel={audioLevel}
              disabled={isAIThinking || processingUserInputRef.current}
            />
          </div>
          
          {/* End call button */}
          <div className="absolute right-4">
            <CallButton 
              type="end_call" 
              onClick={onEndCall} 
            />
          </div>
          
          {/* Speaker control button */}
          <div className="absolute left-4">
            <button
              className={`relative flex items-center justify-center rounded-full p-4 text-white transition-all transform hover:scale-105 active:scale-95
                ${isSpeakerOn ? 'bg-ministry-green shadow-lg shadow-green-500/30' : 'bg-gray-800 hover:bg-gray-700 shadow-md'}`}
              onClick={handleSpeakerClick}
              title={isSpeakerOn ? "Ø¥ÙŠÙ‚Ø§Ù Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª" : "ØªØ´ØºÙŠÙ„ Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª"}
            >
              {isSpeakerOn ? (
                <Volume2 className="h-6 w-6" />
              ) : (
                <Volume className="h-6 w-6" />
              )}
              <span className="sr-only">{isSpeakerOn ? "Ø¥ÙŠÙ‚Ø§Ù Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª" : "ØªØ´ØºÙŠÙ„ Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª"}</span>
              
              {/* Pulse effect when active */}
              {isSpeakerOn && (
                <span className="absolute inset-0 rounded-full bg-ministry-green animate-ping opacity-25"></span>
              )}
            </button>
          </div>
        </div>
      </div>
      
      {/* Hidden audio player with updated muting control */}
      <AudioPlayer 
        audioSource={audioSource}
        audioStream={audioStreamRef.current}
        autoPlay={Boolean(audioSource && isSpeakerOn)}
        onEnded={handleAudioEnded}
        onPlay={() => {
          setIsSpeaking(true);
          console.log("ğŸµ Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª");
        }}
        onError={(e) => {
          console.error("âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª:", e);
          handleAudioEnded();
        }}
        ref={setupAudioController}
        isMuted={audioMuted}
        volume={1.0}
        useStreaming={useStreamingAudio}
      />
    </div>
  );
};

export default ActiveCallScreen;
