
import React, { useState, useRef, useCallback, useEffect } from "react";
import AvatarAnimation from "@/components/AvatarAnimation";
import ChatBubble from "@/components/ChatBubble";
import TranscriptBar from "@/components/TranscriptBar";
import SuggestedQuestions from "@/components/SuggestedQuestions";
import AudioPlayer from "@/components/AudioPlayer";
import ErrorMessage from "@/components/ErrorMessage";
import CallControls from "@/components/CallControls";
import CallStatus from "@/components/CallStatus";
import WelcomeEffectHandler from "@/components/WelcomeEffectHandler";
import { useAIAssistant } from "@/hooks/useAIAssistant";
import { useToast } from "@/hooks/use-toast";
import { useCallMessages } from "@/hooks/useCallMessages";
import { useAudioRecording } from "@/hooks/useAudioRecording";
import { useAudioPlayback } from "@/hooks/useAudioPlayback";

interface ActiveCallScreenProps {
  callStartTime: Date;
  onEndCall: () => void;
}

const ActiveCallScreen: React.FC<ActiveCallScreenProps> = ({ 
  callStartTime, 
  onEndCall 
}) => {
  // State
  const [currentTranscript, setCurrentTranscript] = useState<string>("");
  const [isSpeakerOn, setIsSpeakerOn] = useState<boolean>(true);
  const [audioMuted, setAudioMuted] = useState<boolean>(false);
  const [showErrorMessage, setShowErrorMessage] = useState<boolean>(false);
  const [errorMessage, setErrorMessage] = useState<string>("");
  const [useStreamingAudio, setUseStreamingAudio] = useState<boolean>(true);
  const { toast } = useToast();
  
  // References
  const chatContainerRef = useRef<HTMLDivElement>(null);
  const processingUserInputRef = useRef<boolean>(false);
  const errorTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  
  // The AI assistant hook
  const { 
    askAssistant, 
    cancelRequest,
    isLoading: isAIThinking,
    isAudioLoading 
  } = useAIAssistant();
  
  // Call messages state management
  const { messages, addMessage } = useCallMessages();
  
  // The suggested questions
  const [suggestedQuestions] = useState<string[]>([
    "Ø¥ÙŠÙ‡ Ù‡Ùˆ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªÙƒØ§ÙÙ„ ÙˆÙƒØ±Ø§Ù…Ø©ØŸ",
    "Ø¥Ø²Ø§ÙŠ Ø£Ù‚Ø¯Ù… ÙÙŠ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ØŸ",
    "Ø¥ÙŠÙ‡ Ù‡ÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŸ",
    "Ø£Ù†Ø§ Ø£Ø±Ù…Ù„Ø©.. Ù‡Ù„ Ù…Ù…ÙƒÙ† Ø£Ø³ØªÙÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ØŸ",
    "Ù…ÙˆØ§Ø¹ÙŠØ¯ ØµØ±Ù Ø§Ù„Ø¯Ø¹Ù… Ø¥Ù…ØªÙ‰ØŸ",
    "Ø¹Ù†Ø¯ÙŠ Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙƒØ±Ø§Ù…Ø©",
  ]);

  // Show error message with auto-dismiss
  const showError = useCallback((message: string, duration: number = 5000) => {
    setErrorMessage(message);
    setShowErrorMessage(true);
    
    // Clear any existing timeout
    if (errorTimeoutRef.current) {
      clearTimeout(errorTimeoutRef.current);
    }
    
    // Auto-dismiss after duration
    errorTimeoutRef.current = setTimeout(() => {
      setShowErrorMessage(false);
      setErrorMessage("");
      errorTimeoutRef.current = null;
    }, duration);
  }, []);
  
  // Audio playback hook
  const handleAudioEnded = useCallback(() => {
    setCurrentTranscript(""); // Clear transcript bar text
  }, []);
  
  const { 
    isSpeaking, 
    audioSource, 
    audioStreamRef,
    setupAudioController,
    stopCurrentAudio,
    playTextAsAudio
  } = useAudioPlayback({
    onAudioEnd: handleAudioEnded,
    isSpeakerOn,
    audioMuted
  });

  // Process user input from voice recording or text
  const processUserInput = useCallback(async (text: string) => {
    if (!text.trim() || processingUserInputRef.current) return;
    
    try {
      console.log("ðŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:", text);
      processingUserInputRef.current = true;
      
      // Stop any current audio
      stopCurrentAudio();
      
      // Cancel any pending requests
      cancelRequest?.();
      
      // Hide any error messages
      setShowErrorMessage(false);
      
      // Show current transcript
      setCurrentTranscript(text.trim());
      
      // Add user message
      console.log("ðŸ‘¤ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:", text.trim());
      addMessage(text.trim(), "user");
      
      // Get response from AI assistant
      console.log("ðŸ¤– Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ...");
      const aiResponse = await askAssistant(text.trim());
      
      if (aiResponse) {
        console.log("ðŸ¤– ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ:", aiResponse);
        
        // Add assistant response
        addMessage(aiResponse, "assistant");
        
        // Convert text to speech
        if (!audioMuted && isSpeakerOn) {
          await playTextAsAudio(aiResponse, useStreamingAudio);
        } else {
          // If sound is disabled, skip audio phase
          console.log("ðŸ”‡ ØªØ®Ø·ÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª (Ù…ÙƒØªÙˆÙ… Ø£Ùˆ ØºÙŠØ± Ù†Ø´Ø·)");
          handleAudioEnded();
        }
      } else {
        console.error("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ");
        toast({
          title: "Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯",
          description: "Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
          variant: "destructive",
        });
        handleAudioEnded();
      }
    } catch (error) {
      console.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª:", error);
      toast({
        title: "Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª",
        description: error instanceof Error ? error.message : "Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹",
        variant: "destructive",
      });
      handleAudioEnded();
    } finally {
      processingUserInputRef.current = false;
    }
  }, [addMessage, askAssistant, audioMuted, cancelRequest, handleAudioEnded, isSpeakerOn, playTextAsAudio, stopCurrentAudio, toast, useStreamingAudio]);

  // Audio recording hook
  const { 
    isRecording, 
    audioLevel, 
    startRecording, 
    stopRecording,
    recorderRef
  } = useAudioRecording({
    onTranscriptReady: processUserInput,
    onTranscriptChange: setCurrentTranscript
  });

  // Handle suggested question selection
  const handleQuestionSelect = useCallback((question: string) => {
    console.log("ðŸ–±ï¸ ØªÙ… Ø§Ù„Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø³Ø±ÙŠØ¹:", question);
    if (isSpeaking) {
      console.log("ðŸ›‘ Ø¥ÙŠÙ‚Ø§Ù ÙƒÙ„Ø§Ù… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„");
      stopCurrentAudio();
    }
    
    if (isRecording) {
      console.log("ðŸ›‘ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„");
      recorderRef.current?.cancelRecording();
    }
    
    // Ø¥Ø¶Ø§ÙØ© ØªØ£Ø®ÙŠØ± Ù‚ØµÙŠØ± Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¹Ø§Ø¯Ø© Ø¶Ø¨Ø· ÙƒÙ„ Ø´ÙŠØ¡
    setTimeout(() => {
      processUserInput(question);
    }, 200);
  }, [isSpeaking, isRecording, stopCurrentAudio, recorderRef, processUserInput]);

  // Handle speaker button click - controls audio output
  const handleSpeakerClick = useCallback(() => {
    const newSpeakerState = !isSpeakerOn;
    setIsSpeakerOn(newSpeakerState);
    setAudioMuted(!newSpeakerState);
    
    toast({
      title: newSpeakerState ? "ØªÙ… ØªØ´ØºÙŠÙ„ Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª" : "ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª",
      duration: 2000,
    });
    
    // Stop current audio if turning speaker off
    if (isSpeakerOn && isSpeaking) {
      stopCurrentAudio();
      handleAudioEnded();
    }
  }, [handleAudioEnded, isSpeakerOn, isSpeaking, stopCurrentAudio, toast]);

  // Scroll to bottom when messages change
  useEffect(() => {
    if (chatContainerRef.current) {
      chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight;
    }
  }, [messages]);
  
  // Clean up on unmount
  useEffect(() => {
    return () => {
      if (recorderRef.current && recorderRef.current.isRecording()) {
        recorderRef.current.cancelRecording();
      }
      
      stopCurrentAudio();
      cancelRequest?.();
      
      if (errorTimeoutRef.current) {
        clearTimeout(errorTimeoutRef.current);
      }
    };
  }, [stopCurrentAudio, cancelRequest]);

  // Handle start recording with pre-actions
  const handleStartRecording = useCallback(async () => {
    // If we're already processing or speaking, stop first
    if (processingUserInputRef.current || isSpeaking) {
      console.log("ðŸ›‘ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¬Ø§Ø±ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯");
      stopCurrentAudio();
      cancelRequest?.();
      // Give a moment for cleanup before starting
      await new Promise(resolve => setTimeout(resolve, 100));
    }
    
    // Hide any error messages
    setShowErrorMessage(false);
    
    // Start recording
    await startRecording();
  }, [cancelRequest, isSpeaking, startRecording, stopCurrentAudio]);

  return (
    <div className="relative w-full max-w-md mx-auto aspect-[9/16] md:aspect-auto md:h-[80vh] overflow-hidden rounded-2xl md:rounded-3xl bg-black shadow-xl border-8 border-gray-800 flex flex-col">
      {/* Call background */}
      <div className="absolute inset-0 bg-gradient-to-b from-ministry-dark to-black/90"></div>
      
      {/* Status bar and indicators */}
      <CallStatus 
        callStartTime={callStartTime}
        isAIThinking={isAIThinking}
        isAudioLoading={isAudioLoading}
      />
      
      {/* Error message display */}
      <ErrorMessage 
        showError={showErrorMessage}
        errorMessage={errorMessage}
      />
      
      {/* Chat container - hidden in call UI, used for internal tracking */}
      <div 
        ref={chatContainerRef}
        className="opacity-0 absolute inset-0 overflow-y-auto"
      >
        {messages.map((message) => (
          <ChatBubble
            key={message.id}
            message={message.text}
            sender={message.sender}
          />
        ))}
      </div>
      
      {/* Animated avatar - center of screen */}
      <div className="absolute inset-0 flex items-center justify-center pointer-events-none z-10">
        <AvatarAnimation 
          isActive={isSpeaking} 
          isListening={isRecording}
          audioLevel={audioLevel}
        />
      </div>
      
      {/* Current transcript bar - under chin */}
      <div className="absolute bottom-32 left-0 right-0 z-20 px-4">
        <TranscriptBar 
          text={currentTranscript} 
          isActive={Boolean(currentTranscript)} 
          autoHide={false}
        />
      </div>
      
      {/* Suggested questions bar */}
      <div className="absolute bottom-24 left-0 right-0 z-20 px-2">
        <SuggestedQuestions 
          questions={suggestedQuestions} 
          onQuestionSelect={handleQuestionSelect} 
        />
      </div>
      
      {/* Call controls (push to talk, speaker, end call) */}
      <CallControls 
        isSpeakerOn={isSpeakerOn}
        isRecording={isRecording}
        audioLevel={audioLevel}
        processingUserInput={processingUserInputRef.current}
        isAIThinking={isAIThinking}
        onSpeakerToggle={handleSpeakerClick}
        onStartRecording={handleStartRecording}
        onStopRecording={() => stopRecording(false)}
        onEndCall={onEndCall}
      />
      
      {/* Welcome message handler */}
      <WelcomeEffectHandler
        isSpeakerOn={isSpeakerOn}
        playTextAsAudio={playTextAsAudio}
        useStreamingAudio={useStreamingAudio}
      />
      
      {/* Hidden audio player with updated muting control */}
      <AudioPlayer 
        audioSource={audioSource}
        audioStream={audioStreamRef.current}
        autoPlay={Boolean(audioSource && isSpeakerOn)}
        onEnded={handleAudioEnded}
        onPlay={() => {
          console.log("ðŸŽµ Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª");
        }}
        onError={(e) => {
          console.error("âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª:", e);
          handleAudioEnded();
        }}
        ref={setupAudioController}
        isMuted={audioMuted}
        volume={1.0}
        useStreaming={useStreamingAudio}
      />
    </div>
  );
};

export default ActiveCallScreen;
