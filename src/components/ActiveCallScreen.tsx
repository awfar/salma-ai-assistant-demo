
import React, { useState, useEffect, useRef, useCallback } from "react";
import { Volume2, Volume } from "lucide-react";
import CallTimer from "@/components/CallTimer";
import AudioPlayer from "@/components/AudioPlayer";
import AvatarAnimation from "@/components/AvatarAnimation";
import CallButton from "@/components/CallButton";
import SoundWave from "@/components/SoundWave";
import ChatBubble from "@/components/ChatBubble";
import TranscriptBar from "@/components/TranscriptBar";
import SuggestedQuestions from "@/components/SuggestedQuestions";
import { useAIAssistant } from "@/hooks/useAIAssistant";
import { useSpeechRecognition } from "@/hooks/useSpeechRecognition";
import { useToast } from "@/hooks/use-toast";
import { useCallMessages } from "@/hooks/useCallMessages";

interface ActiveCallScreenProps {
  callStartTime: Date;
  onEndCall: () => void;
}

const ActiveCallScreen: React.FC<ActiveCallScreenProps> = ({ 
  callStartTime, 
  onEndCall 
}) => {
  const [isSpeaking, setIsSpeaking] = useState<boolean>(false);
  const [isMuted, setIsMuted] = useState<boolean>(false);
  const [audioSource, setAudioSource] = useState<string | undefined>();
  const [currentTranscript, setCurrentTranscript] = useState<string>("");
  const [isSpeakerOn, setIsSpeakerOn] = useState<boolean>(true);
  const { toast } = useToast();
  
  // References
  const chatContainerRef = useRef<HTMLDivElement>(null);
  const audioControllerRef = useRef<{ 
    pause: () => void;
    play: () => Promise<void>;
    isPlaying: boolean; 
  } | null>(null);
  const firstMessagePlayed = useRef<boolean>(false);
  const autoListenTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const maxListeningTimeRef = useRef<NodeJS.Timeout | null>(null);
  const lastAudioLevelTimestampRef = useRef<number>(Date.now());
  
  // The AI assistant hook
  const { 
    askAssistant, 
    textToSpeech, 
    isLoading: isAIThinking,
    isAudioLoading 
  } = useAIAssistant();
  
  // Call messages state management
  const { messages, addMessage } = useCallMessages();
  
  // The suggested questions
  const [suggestedQuestions] = useState<string[]>([
    "Ø¥ÙŠÙ‡ Ù‡Ùˆ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªÙƒØ§ÙÙ„ ÙˆÙƒØ±Ø§Ù…Ø©ØŸ",
    "Ø¥Ø²Ø§ÙŠ Ø£Ù‚Ø¯Ù… ÙÙŠ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ØŸ",
    "Ø¥ÙŠÙ‡ Ù‡ÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŸ",
    "Ø£Ù†Ø§ Ø£Ø±Ù…Ù„Ø©.. Ù‡Ù„ Ù…Ù…ÙƒÙ† Ø£Ø³ØªÙÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ØŸ",
    "Ù…ÙˆØ§Ø¹ÙŠØ¯ ØµØ±Ù Ø§Ù„Ø¯Ø¹Ù… Ø¥Ù…ØªÙ‰ØŸ",
    "Ø¹Ù†Ø¯ÙŠ Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙƒØ±Ø§Ù…Ø©",
  ]);

  // Process user input (from voice or button)
  const processUserInput = async (text: string) => {
    if (!text.trim()) return;
    
    // Show current transcript
    setCurrentTranscript(text.trim());
    
    // Add user message
    console.log("ğŸ‘¤ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:", text.trim());
    addMessage(text.trim(), "user");
    resetTranscript();
    
    // Get response from AI assistant
    console.log("ğŸ¤– Ø¬Ø§Ø±ÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ...");
    const aiResponse = await askAssistant(text.trim());
    
    if (aiResponse) {
      console.log("ğŸ¤– ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ:", aiResponse);
      
      // Add assistant response
      addMessage(aiResponse, "assistant");
      
      // Convert text to speech
      if (!isMuted && isSpeakerOn) {
        console.log("ğŸ”Š Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…...");
        const audioUrl = await textToSpeech(aiResponse, {
          onStart: () => {
            console.log("ğŸ”Š Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª");
            setIsSpeaking(true);
          },
          onEnd: () => {
            console.log("ğŸ”Š Ø§Ù†ØªÙ‡Ø§Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª");
            setIsSpeaking(false);
          }
        });
        
        if (audioUrl) {
          console.log("ğŸ”Š ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØª:", audioUrl.substring(0, 50) + "...");
          setAudioSource(audioUrl);
        } else {
          console.error("âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ URL Ù„Ù„ØµÙˆØª");
          handleAudioEnded(); // Call this to ensure program flow continues
        }
      } else {
        // If sound is disabled, skip audio phase
        console.log("ğŸ”‡ ØªØ®Ø·ÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª (Ù…ÙƒØªÙˆÙ… Ø£Ùˆ ØºÙŠØ± Ù†Ø´Ø·)");
        handleAudioEnded();
      }
    } else {
      console.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ");
      toast({
        title: "Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯",
        description: "Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
        variant: "destructive",
      });
      
      // Start listening again
      if (!isMuted) {
        scheduleListening(1000);
      }
    }
  };

  // Speech recognition handling - directly use processUserInput
  const handleTranscriptResult = (text: string) => {
    processUserInput(text);
  };

  // Reset silence detection when audio level changes
  const handleAudioLevelChange = useCallback((level: number) => {
    // If we detect sound above threshold, reset silence timeout
    if (level > 0.05) {
      lastAudioLevelTimestampRef.current = Date.now();
      
      // Clear existing silence timeout if there is one
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current);
        silenceTimeoutRef.current = null;
      }
    } else if (!silenceTimeoutRef.current) {
      // No significant audio for a while, start silence timeout
      const timeSinceLastSound = Date.now() - lastAudioLevelTimestampRef.current;
      
      // Only set silence timeout if it's been silent for a bit
      if (timeSinceLastSound > 300) {
        silenceTimeoutRef.current = setTimeout(() => {
          console.log("ğŸ”‡ ØªÙ… Ø§ÙƒØªØ´Ø§Ù ØµÙ…Øª Ù„Ù…Ø¯Ø© Ø·ÙˆÙŠÙ„Ø©ØŒ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹");
          if (isListening) {
            stopListening();
          }
          silenceTimeoutRef.current = null;
        }, 1500); // Stop after 1.5 seconds of silence
      }
    }
  }, []);
  
  // Speech recognition hook with silence detection
  const { 
    isListening,
    startListening,
    stopListening,
    transcript,
    isProcessing: isTranscribing,
    error: speechError,
    resetTranscript,
    audioLevel
  } = useSpeechRecognition({
    onResult: handleTranscriptResult,
    onListeningChange: (listening) => {
      console.log("ğŸ¤ Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹:", listening ? "Ù†Ø´Ø·" : "Ù…ØªÙˆÙ‚Ù");
    },
    onProcessingChange: (processing) => {
      console.log("ğŸ¤ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:", processing ? "Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©" : "Ù…ØªÙˆÙ‚Ù");
    },
    onAudioLevelChange: handleAudioLevelChange
  });

  // Update transcript and monitor audio level during listening
  useEffect(() => {
    if (isListening) {
      // Update current transcript during listening
      if (transcript) {
        setCurrentTranscript(transcript);
      }
      
      // Set maximum listening time (8 seconds) as safety
      if (!maxListeningTimeRef.current) {
        maxListeningTimeRef.current = setTimeout(() => {
          console.log("â±ï¸ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ØŒ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹");
          if (isListening) {
            stopListening();
          }
          maxListeningTimeRef.current = null;
        }, 8000);
      }
    } else {
      // Clear timeout when not listening
      if (maxListeningTimeRef.current) {
        clearTimeout(maxListeningTimeRef.current);
        maxListeningTimeRef.current = null;
      }
    }
  }, [transcript, isListening, stopListening]);

  // Schedule listening after a delay
  const scheduleListening = (delay: number = 500) => {
    if (autoListenTimeoutRef.current) {
      clearTimeout(autoListenTimeoutRef.current);
    }
    
    autoListenTimeoutRef.current = setTimeout(() => {
      if (!isListening && !isTranscribing && !isAIThinking && !isSpeaking && !isMuted) {
        console.log("ğŸ”„ Ø¬Ø¯ÙˆÙ„Ø© Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§");
        startListening();
      }
    }, delay);
  };

  // Handle suggested question selection - use processUserInput directly
  const handleQuestionSelect = (question: string) => {
    if (isSpeaking || isTranscribing || isAIThinking || isListening) {
      console.log("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„Ø¢Ù†:", {
        isSpeaking,
        isTranscribing,
        isAIThinking,
        isListening
      });
      return;
    }
    
    // Stop current listening if any
    if (isListening) {
      stopListening();
    }
    
    console.log("ğŸ“ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„ Ù…Ù‚ØªØ±Ø­:", question);
    
    // Process the question directly using the same pipeline as voice input
    processUserInput(question);
  };

  // When audio ends, start listening automatically
  const handleAudioEnded = useCallback(() => {
    setIsSpeaking(false);
    setCurrentTranscript(""); // Clear transcript bar text
    
    if (!isMuted && firstMessagePlayed.current) {
      // Short delay before starting to listen
      scheduleListening(800);
    }
  }, [isMuted]);

  // Stop current audio
  const stopCurrentAudio = useCallback(() => {
    if (audioControllerRef.current) {
      audioControllerRef.current.pause();
    }
    setIsSpeaking(false);
  }, []);

  // Handle mute button click
  const handleMuteClick = () => {
    setIsMuted(!isMuted);
    
    // If muting, stop current audio and listening
    if (!isMuted) {
      stopCurrentAudio();
      if (isListening) {
        stopListening();
      }
      
      toast({
        title: "ØªÙ… ÙƒØªÙ… Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†",
        duration: 2000,
      });
    } else {
      toast({
        title: "ØªÙ… ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†",
        duration: 2000,
      });
      
      // If unmuting and no audio is playing, start listening
      if (!isSpeaking && !isListening && !isTranscribing && !isAIThinking) {
        scheduleListening(800);
      }
    }
  };

  // Handle speaker button click
  const handleSpeakerClick = () => {
    setIsSpeakerOn(!isSpeakerOn);
    toast({
      title: isSpeakerOn ? "ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª" : "ØªÙ… ØªØ´ØºÙŠÙ„ Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª",
      duration: 2000,
    });
    
    // Stop current audio if turning speaker off
    if (isSpeakerOn && isSpeaking) {
      stopCurrentAudio();
      handleAudioEnded();
    }
  };

  // Setup audio controller reference
  const setupAudioController = useCallback((controller: { 
    pause: () => void;
    play: () => Promise<void>;
    isPlaying: boolean; 
  } | null) => {
    audioControllerRef.current = controller;
  }, []);

  // Scroll to bottom when messages change
  useEffect(() => {
    if (chatContainerRef.current) {
      chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight;
    }
  }, [messages]);
  
  // Stop listening when stopping call
  useEffect(() => {
    if (isListening) {
      stopListening();
    }
    stopCurrentAudio();
    
    // Clean up all timeouts
    if (autoListenTimeoutRef.current) {
      clearTimeout(autoListenTimeoutRef.current);
    }
    
    if (silenceTimeoutRef.current) {
      clearTimeout(silenceTimeoutRef.current);
    }
    
    if (maxListeningTimeRef.current) {
      clearTimeout(maxListeningTimeRef.current);
    }
  }, [isListening, stopListening, stopCurrentAudio]);

  // Stop listening if AI is thinking or speaking
  useEffect(() => {
    if ((isAIThinking || isSpeaking) && isListening) {
      stopListening();
    }
  }, [isAIThinking, isSpeaking, isListening, stopListening]);

  // Retry on speech recognition error
  useEffect(() => {
    if (speechError && !isSpeaking && !isListening && !isTranscribing && !isAIThinking) {
      console.log("Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ø¨Ø¹Ø¯ Ø§Ù„Ø®Ø·Ø£:", speechError);
      scheduleListening(2000);
    }
  }, [speechError, isSpeaking, isListening, isTranscribing, isAIThinking]);

  // Clean up timers when unmounting
  useEffect(() => {
    return () => {
      if (autoListenTimeoutRef.current) {
        clearTimeout(autoListenTimeoutRef.current);
      }
      
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current);
      }
      
      if (maxListeningTimeRef.current) {
        clearTimeout(maxListeningTimeRef.current);
      }
    };
  }, []);

  // Retry playing audio if source exists but not playing
  useEffect(() => {
    if (audioSource && !isSpeaking && !isMuted && isSpeakerOn && audioControllerRef.current) {
      // Short delay then try playing audio again if not already playing
      const timer = setTimeout(() => {
        if (audioControllerRef.current && !audioControllerRef.current.isPlaying) {
          console.log("ğŸ”„ Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª");
          audioControllerRef.current.play().catch(() => {
            console.error("âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª");
            handleAudioEnded();
          });
        }
      }, 500);
      
      return () => clearTimeout(timer);
    }
  }, [audioSource, isSpeaking, isMuted, isSpeakerOn, handleAudioEnded]);

  // Pre-initialize microphone
  useEffect(() => {
    // Pre-request microphone permissions
    navigator.mediaDevices.getUserMedia({ 
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      } 
    })
      .then(() => console.log("âœ… ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù…Ø³Ø¨Ù‚Ù‹Ø§"))
      .catch(err => console.error("âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†:", err));
  }, []);

  // Play welcome message on first render - ONLY ONCE
  useEffect(() => {
    // Play welcome message after a short delay
    const welcomeTimer = setTimeout(async () => {
      if (firstMessagePlayed.current) {
        // If we've already played the welcome message, just start listening
        scheduleListening(800);
        return;
      }
      
      const welcomeMessage = "Ø£Ù‡Ù„Ø§ Ø¨ÙŠÙƒ ÙÙŠ ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¶Ø§Ù…Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŒ Ù…Ø¹Ø§Ùƒ Ø³Ù„Ù…Ù‰ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„Ø°ÙƒÙŠØ© Ø£Ù†Ø§ Ù‡Ù†Ø§ Ø¹Ø´Ø§Ù† Ø§Ø¬Ø§ÙˆØ¨Ùƒ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ø²Ø§ÙŠ Ø§Ù‚Ø¯Ø± Ø§Ø³Ø§Ø¹Ø¯ÙƒØŸ";
      console.log("ğŸ¤– Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨:", welcomeMessage);
      addMessage(welcomeMessage, "assistant");
      
      // Convert text to speech
      if (isSpeakerOn) {
        console.log("ğŸ”Š Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨ Ø¥Ù„Ù‰ ØµÙˆØª...");
        const audioUrl = await textToSpeech(welcomeMessage, {
          onStart: () => setIsSpeaking(true),
          onEnd: () => setIsSpeaking(false)
        });
        
        if (audioUrl) {
          setAudioSource(audioUrl);
          // Mark first message as played to prevent repeating
          firstMessagePlayed.current = true;
        } else {
          // If text-to-speech fails, mark as played and start listening
          firstMessagePlayed.current = true;
          handleAudioEnded();
        }
      } else {
        firstMessagePlayed.current = true;
        handleAudioEnded();
      }
    }, 500);
    
    return () => clearTimeout(welcomeTimer);
  }, []);

  return (
    <div className="relative w-full max-w-md mx-auto aspect-[9/16] md:aspect-auto md:h-[80vh] overflow-hidden rounded-2xl md:rounded-3xl bg-black shadow-xl border-8 border-gray-800 flex flex-col">
      {/* Call background */}
      <div className="absolute inset-0 bg-gradient-to-b from-ministry-dark to-black/90"></div>
      
      {/* Status bar at top */}
      <div className="absolute top-0 left-0 right-0 h-12 bg-black/30 backdrop-blur-lg flex items-center justify-center z-10">
        <CallTimer isActive={true} startTime={callStartTime} className="text-white font-bold" />
      </div>
      
      {/* Chat container - hidden in call UI, used for internal tracking */}
      <div 
        ref={chatContainerRef}
        className="opacity-0 absolute inset-0 overflow-y-auto"
      >
        {messages.map((message) => (
          <ChatBubble
            key={message.id}
            message={message.text}
            sender={message.sender}
          />
        ))}
      </div>
      
      {/* Animated avatar - center of screen */}
      <div className="absolute inset-0 flex items-center justify-center pointer-events-none z-10">
        <AvatarAnimation 
          isActive={isSpeaking} 
          isListening={!isSpeaking && isListening}
          audioLevel={audioLevel}
        />
      </div>
      
      {/* Current transcript bar - under chin */}
      <div className="absolute bottom-32 left-0 right-0 z-20 px-4">
        <TranscriptBar 
          text={currentTranscript} 
          isActive={Boolean(isSpeaking || (isListening && transcript))} 
          autoHide={false}
        />
      </div>
      
      {/* Suggested questions bar */}
      <div className="absolute bottom-24 left-0 right-0 z-20 px-2">
        <SuggestedQuestions 
          questions={suggestedQuestions} 
          onQuestionSelect={handleQuestionSelect} 
        />
      </div>
      
      {/* Status indicators */}
      {isAIThinking && (
        <div className="absolute bottom-40 left-0 right-0 z-10 flex justify-center">
          <div className="bg-ministry-dark/50 backdrop-blur-sm px-4 py-2 rounded-full flex items-center gap-2">
            <div className="text-white text-xs">Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙÙƒÙŠØ±</div>
            <div className="flex space-x-1 rtl:space-x-reverse">
              {Array.from({ length: 3 }).map((_, i) => (
                <div 
                  key={i} 
                  className="w-1.5 h-1.5 bg-gray-400 rounded-full animate-pulse"
                  style={{ animationDelay: `${i * 200}ms` }}
                ></div>
              ))}
            </div>
          </div>
        </div>
      )}
      
      {isTranscribing && (
        <div className="absolute bottom-40 left-0 right-0 z-10 flex justify-center">
          <div className="bg-ministry-dark/50 backdrop-blur-sm px-4 py-2 rounded-full flex items-center gap-2">
            <div className="text-white text-xs">Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª</div>
            <SoundWave isActive={true} type="listening" className="h-4 w-16" />
          </div>
        </div>
      )}
      
      {isListening && (
        <div className="absolute top-16 right-4 flex items-center gap-2 animate-pulse">
          <div className="flex space-x-1 rtl:space-x-reverse">
            {Array.from({ length: 3 }).map((_, i) => (
              <div 
                key={i} 
                className="w-2 h-2 bg-green-500 rounded-full animate-pulse"
                style={{ animationDelay: `${i * 200}ms` }}
              ></div>
            ))}
          </div>
          <span className="text-xs text-white bg-green-500/80 px-2 py-0.5 rounded-full">Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹</span>
        </div>
      )}
      
      {/* Audio processing icon */}
      {isAudioLoading && (
        <div className="absolute top-16 left-4 flex items-center gap-2 animate-pulse">
          <div className="flex space-x-1 rtl:space-x-reverse">
            <div className="w-2 h-2 bg-blue-400 rounded-full animate-pulse"></div>
            <div className="w-2 h-2 bg-blue-400 rounded-full animate-pulse" style={{ animationDelay: '200ms' }}></div>
            <div className="w-2 h-2 bg-blue-400 rounded-full animate-pulse" style={{ animationDelay: '400ms' }}></div>
          </div>
          <span className="text-xs text-white bg-blue-400/80 px-2 py-0.5 rounded-full">Ø¬Ø§Ø±ÙŠ ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙˆØª</span>
        </div>
      )}
      
      {/* Call control buttons - iOS style */}
      <div className="absolute bottom-4 left-0 right-0 z-30">
        <div className="flex items-center justify-center space-x-5 rtl:space-x-reverse">
          <CallButton 
            type="mute" 
            onClick={handleMuteClick} 
            active={isMuted}
          />
          <CallButton 
            type="end_call" 
            onClick={onEndCall} 
            className="p-5" 
          />
          {/* Speaker control button */}
          <button
            className={`relative flex items-center justify-center rounded-full p-4 text-white transition-all transform hover:scale-105 active:scale-95
              ${isSpeakerOn ? 'bg-ministry-green shadow-lg shadow-green-500/30' : 'bg-gray-800 hover:bg-gray-700 shadow-md'}`}
            onClick={handleSpeakerClick}
            title={isSpeakerOn ? "Ø¥ÙŠÙ‚Ø§Ù Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª" : "ØªØ´ØºÙŠÙ„ Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª"}
          >
            {isSpeakerOn ? (
              <Volume2 className="h-6 w-6" />
            ) : (
              <Volume className="h-6 w-6" />
            )}
            <span className="sr-only">{isSpeakerOn ? "Ø¥ÙŠÙ‚Ø§Ù Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª" : "ØªØ´ØºÙŠÙ„ Ù…ÙƒØ¨Ø± Ø§Ù„ØµÙˆØª"}</span>
            
            {/* Pulse effect when active */}
            {isSpeakerOn && (
              <span className="absolute inset-0 rounded-full bg-ministry-green animate-ping opacity-25"></span>
            )}
          </button>
        </div>
      </div>
      
      {/* Hidden audio player */}
      <AudioPlayer 
        audioSource={audioSource} 
        autoPlay={Boolean(isSpeakerOn && !isMuted)}
        onEnded={handleAudioEnded}
        onPlay={() => {
          setIsSpeaking(true);
          console.log("ğŸµ Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª");
        }}
        onError={(e) => {
          console.error("âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª:", e);
          handleAudioEnded();
        }}
        ref={setupAudioController}
      />
    </div>
  );
};

export default ActiveCallScreen;
